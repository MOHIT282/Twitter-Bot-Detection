{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f0da3da",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c364d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy_language_detection import LanguageDetector\n",
    "\n",
    "from collections import Counter\n",
    "import string\n",
    "# from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "PUNCTUATION = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ead81659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1f515c0da90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48990eca",
   "metadata": {},
   "source": [
    "### Reading the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9759f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status_id</th>\n",
       "      <th>account.type</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1208265880146046976</td>\n",
       "      <td>bot</td>\n",
       "      <td>YEA now that note GOOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1091463908118941696</td>\n",
       "      <td>human</td>\n",
       "      <td>Listen to This Charming Man by The Smiths  htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1199055191028293633</td>\n",
       "      <td>bot</td>\n",
       "      <td>たぶんあの時からわたしは……そなたが</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1214698264701722626</td>\n",
       "      <td>bot</td>\n",
       "      <td>The decade in the significantly easier schedul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1209229478934695937</td>\n",
       "      <td>bot</td>\n",
       "      <td>\"Theim class=\\\"alignnone size-full wp-image-60...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             status_id account.type  \\\n",
       "0  1208265880146046976          bot   \n",
       "1  1091463908118941696        human   \n",
       "2  1199055191028293633          bot   \n",
       "3  1214698264701722626          bot   \n",
       "4  1209229478934695937          bot   \n",
       "\n",
       "                                              tweets  \n",
       "0                             YEA now that note GOOD  \n",
       "1  Listen to This Charming Man by The Smiths  htt...  \n",
       "2                                 たぶんあの時からわたしは……そなたが  \n",
       "3  The decade in the significantly easier schedul...  \n",
       "4  \"Theim class=\\\"alignnone size-full wp-image-60...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"twitter.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e1e5ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c2e1f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['status_id', 'account.type', 'tweets'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64ecac6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "status_id       object\n",
       "account.type    object\n",
       "tweets          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d732188",
   "metadata": {},
   "source": [
    "#### Checking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea649a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "status_id       0\n",
       "account.type    0\n",
       "tweets          4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8134aa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78c6a45e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30f6bc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns = {\"account.type\":\"account_type\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67348230",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a5de583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "account_type\n",
       "human    5002\n",
       "bot      4994\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.account_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52979850",
   "metadata": {},
   "source": [
    "#### text normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2651b14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"tweets\"] = data[\"tweets\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6984d46c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status_id</th>\n",
       "      <th>account_type</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1208265880146046976</td>\n",
       "      <td>bot</td>\n",
       "      <td>yea now that note good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1091463908118941696</td>\n",
       "      <td>human</td>\n",
       "      <td>listen to this charming man by the smiths  htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1199055191028293633</td>\n",
       "      <td>bot</td>\n",
       "      <td>たぶんあの時からわたしは……そなたが</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1214698264701722626</td>\n",
       "      <td>bot</td>\n",
       "      <td>the decade in the significantly easier schedul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1209229478934695937</td>\n",
       "      <td>bot</td>\n",
       "      <td>\"theim class=\\\"alignnone size-full wp-image-60...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             status_id account_type  \\\n",
       "0  1208265880146046976          bot   \n",
       "1  1091463908118941696        human   \n",
       "2  1199055191028293633          bot   \n",
       "3  1214698264701722626          bot   \n",
       "4  1209229478934695937          bot   \n",
       "\n",
       "                                              tweets  \n",
       "0                             yea now that note good  \n",
       "1  listen to this charming man by the smiths  htt...  \n",
       "2                                 たぶんあの時からわたしは……そなたが  \n",
       "3  the decade in the significantly easier schedul...  \n",
       "4  \"theim class=\\\"alignnone size-full wp-image-60...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89a9faf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify(q):\n",
    "    contractions = { \n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"can't've\": \"can not have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he will have\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "    }\n",
    "    q_decontracted = []\n",
    "\n",
    "    for word in q.split():\n",
    "        if word in contractions:\n",
    "            word = contractions[word]\n",
    "\n",
    "        q_decontracted.append(word)\n",
    "\n",
    "    q = ' '.join(q_decontracted)\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3529960b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"tweets\"] = data[\"tweets\"].apply(lambda i: simplify(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49cce92",
   "metadata": {},
   "source": [
    "#### Detecting non english languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f117d0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lang_detector(nlp, name):\n",
    "    return LanguageDetector(seed=42)  # We use the seed 42\n",
    "\n",
    "nlp_model = spacy.load(\"en_core_web_sm\")\n",
    "Language.factory(\"language_detector\", func=get_lang_detector)\n",
    "nlp_model.add_pipe('language_detector', last=True)\n",
    "\n",
    "def Language_Detection(txt):\n",
    "    doc = nlp_model(txt)\n",
    "    return (doc._.language['language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f308635",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lan'] = data['tweets'].apply(lambda x: Language_Detection(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3227ec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "inx = data[data['lan']!='en'].index\n",
    "data.drop(inx, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6651c423",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['account_type'] = data['account_type'].map({'human': 1, 'bot': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e680c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(data, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9855ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_df['tweets']\n",
    "train_target = train_df['account_type']\n",
    "\n",
    "test_data = test_df['tweets']\n",
    "test_target = test_df['account_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173c99b6",
   "metadata": {},
   "source": [
    "#### Tf-Idf pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cafdae8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.71       353\n",
      "           1       0.71      0.58      0.64       345\n",
      "\n",
      "    accuracy                           0.68       698\n",
      "   macro avg       0.68      0.68      0.67       698\n",
      "weighted avg       0.68      0.68      0.67       698\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bow_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"classifier\", RandomForestClassifier()),\n",
    "    ]\n",
    ")\n",
    "bow_pipeline.fit(train_data, train_target)\n",
    "y_pred = bow_pipeline.predict(test_data)\n",
    "cr = classification_report(test_target, y_pred)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f42493",
   "metadata": {},
   "source": [
    "#### Word embeddings pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf2e4f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "nlp = spacy.load(\"en_core_web_sm\")  # this model will give you 300D\n",
    "class SpacyVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "        self.dim = 300\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Doc.vector defaults to an average of the token vectors.\n",
    "        # https://spacy.io/api/doc#vector\n",
    "        return [self.nlp(text).vector for text in X]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f491452c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       0.67      0.70      0.68       353\\n           1       0.68      0.65      0.66       345\\n\\n    accuracy                           0.67       698\\n   macro avg       0.67      0.67      0.67       698\\nweighted avg       0.67      0.67      0.67       698\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"mean_embeddings\", SpacyVectorTransformer(nlp)),\n",
    "        (\"classifier\", RandomForestClassifier()),\n",
    "    ]\n",
    ")\n",
    "embeddings_pipeline.fit(train_data, train_target)\n",
    "y_pred = embeddings_pipeline.predict(test_data)\n",
    "cr = classification_report(test_target, y_pred)\n",
    "cr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c325f798",
   "metadata": {},
   "source": [
    "#### Custom Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d67ae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentFeaturizer:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.future_words = [\"tomorrow\", \"future\", \"futures\"]\n",
    "        \n",
    "    @staticmethod\n",
    "    def getCharCount(doc):\n",
    "#         print(len(doc.text))\n",
    "        return len(doc.text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def getWordsInSen(doc):\n",
    "        res = sum([i.strip(string.punctuation).isalpha() for i in doc.text.split()])\n",
    "        return res\n",
    "    \n",
    "    @staticmethod\n",
    "    def noSen(doc):\n",
    "        return len(nltk.sent_tokenize(doc.text))\n",
    "    \n",
    "    @staticmethod\n",
    "    def noUniqueWord(doc):\n",
    "        return len(set(doc.text.split()))\n",
    "    \n",
    "    @staticmethod\n",
    "    def avgWordLenInSen(doc):\n",
    "        words = [word for word in doc.text.split() if word]\n",
    "        if (len(words) != 0):\n",
    "            avg = sum(map(len, words))/len(words)\n",
    "            return avg\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    @staticmethod\n",
    "    def avgSenLen(doc):\n",
    "        if (SegmentFeaturizer.noSen(doc) != 0):\n",
    "            return (SegmentFeaturizer.getWordsInSen(doc) / SegmentFeaturizer.noSen(doc))\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    @staticmethod\n",
    "    def count_pronouns(doc):\n",
    "        segment = doc.text.lower().split()\n",
    "        counter = {\"1sg\": 0, \"1pl\": 0}\n",
    "        for pronoun in FIRST_SINGULAR:\n",
    "            counter[\"1sg\"] += segment.count(pronoun)\n",
    "        for pronoun in FIRST_PLURAL:\n",
    "            counter[\"1pl\"] += segment.count(pronoun)\n",
    "        return counter\n",
    "\n",
    "    @staticmethod\n",
    "    def getWordInDoubleQuote(doc):\n",
    "        return len(re.findall(r'[\"][\\w\\s]+[\"]', doc.text))\n",
    "    \n",
    "    @staticmethod\n",
    "    def getNoStopWords(doc):\n",
    "        return len([x for x in nltk.word_tokenize(doc.text) if x in STOPWORDS])\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def getPunctuationcount(doc):\n",
    "        return len(\"\".join(x for x in doc.text if x in PUNCTUATION))\n",
    "    \n",
    "    @staticmethod\n",
    "    def getNoPositiveWords(doc):\n",
    "        return len([i for i in doc.text.split() if TextBlob(i).sentiment.polarity >= 0.5])\n",
    "    \n",
    "    @staticmethod\n",
    "    def getNoNegativeWords(doc):\n",
    "        return len([i for i in doc.text.split() if TextBlob(i).sentiment.polarity >= 0.5])\n",
    "\n",
    "    @staticmethod\n",
    "    def get_n_words_before_main_verb(doc):\n",
    "        numbers = [0]\n",
    "        for sent in doc.sents:\n",
    "            main = [t for t in sent if t.dep_ == \"ROOT\"][0]\n",
    "            if main.pos_ == \"VERB\":\n",
    "                dist_to_init = main.i - sent[0].i\n",
    "                numbers.append(dist_to_init)\n",
    "        return np.mean(numbers)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_n_complex_clauses(doc):\n",
    "        embedded_elements_count = []\n",
    "        for sent in doc.sents:\n",
    "            n_embedded = len(\n",
    "                [t for t in sent if t.dep_ in {\"ccomp\", \"xcomp\", \"advcl\", \"dative\"}]\n",
    "            )\n",
    "            embedded_elements_count.append(n_embedded)\n",
    "        return np.mean(embedded_elements_count)\n",
    "    \n",
    "    # putting it all together!\n",
    "    def featurize(self, segments):\n",
    "        feature_dicts = []\n",
    "        docs = self.nlp.pipe(segments)\n",
    "        for doc in docs:\n",
    "            feature_dict = {\n",
    "                'charCount': self.getCharCount(doc),\n",
    "                'getWordsInSen': self.getWordsInSen(doc),\n",
    "                'noSen': self.noSen(doc),\n",
    "                'noUniqueWord': self.noUniqueWord(doc),\n",
    "                'avgWordLenInSen': self.avgWordLenInSen(doc),\n",
    "                'avgSenLen': self.avgSenLen(doc),\n",
    "                \"n_complex_clauses\": self.get_n_complex_clauses(doc),\n",
    "                \"n_words_before_main_verb\": self.get_n_words_before_main_verb(doc),\n",
    "                'getNoNegativeWords': self.getNoNegativeWords(doc),\n",
    "                'getNoPositiveWords': self.getNoPositiveWords(doc),\n",
    "                'getPunctuationcount': self.getPunctuationcount(doc),\n",
    "                'getNoStopWords': self.getNoStopWords(doc),\n",
    "                'getWordInDoubleQuote': self.getWordInDoubleQuote(doc),\n",
    "                \n",
    "            }\n",
    "            feature_dicts.append(feature_dict)\n",
    "        return feature_dicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e278b301",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_featurizer = SegmentFeaturizer()  # more on this below\n",
    "class CustomLinguisticFeatureTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.l = 1\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, data):\n",
    "        return segment_featurizer.featurize(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50178c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       0.68      0.72      0.70       353\\n           1       0.70      0.66      0.68       345\\n\\n    accuracy                           0.69       698\\n   macro avg       0.69      0.69      0.69       698\\nweighted avg       0.69      0.69      0.69       698\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"stats\", CustomLinguisticFeatureTransformer()),\n",
    "        (\"dict_vect\", DictVectorizer()),\n",
    "        (\"classifier\", RandomForestClassifier()),\n",
    "    ]\n",
    ")\n",
    "manual_pipeline.fit(train_data, train_target)\n",
    "y_pred = manual_pipeline.predict(test_data)\n",
    "cr = classification_report(test_target, y_pred)\n",
    "cr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f45d22",
   "metadata": {},
   "source": [
    "#### Combined pipeline - tfidf pipeline + custom pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3355559",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "    ]\n",
    ")\n",
    "manual_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"stats\", CustomLinguisticFeatureTransformer()),\n",
    "        (\"dict_vect\", DictVectorizer()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce559170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "combined_features = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        (\"manual\", manual_pipeline),\n",
    "        (\"bow\", bow_pipeline),\n",
    "    ]\n",
    ")\n",
    "final_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"combined_features\", combined_features),\n",
    "        (\"classifier\", RandomForestClassifier()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f26a09bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       0.70      0.79      0.74       353\\n           1       0.76      0.66      0.70       345\\n\\n    accuracy                           0.72       698\\n   macro avg       0.73      0.72      0.72       698\\nweighted avg       0.73      0.72      0.72       698\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pipeline.fit(train_data, train_target)\n",
    "y_pred = final_pipeline.predict(test_data)\n",
    "cr = classification_report(test_target, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55dca647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7249283667621776"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_target, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5279cb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.74       353\n",
      "           1       0.76      0.66      0.70       345\n",
      "\n",
      "    accuracy                           0.72       698\n",
      "   macro avg       0.73      0.72      0.72       698\n",
      "weighted avg       0.73      0.72      0.72       698\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c94a8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(final_pipeline, open('custom1_pipe_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bc901a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
